
ANT: Another Neural Tool
<p>
This is a minimal Neural Network library, written in C++, with the object of being easily understood.  It is a demonstration of implementation, consciously eschewing opacity in favour of plain and comprehensible code.  There are also examples of how to verify an implementation by reconciling gradients when training; this is an oft ignored aspect of demonstrating ANNs.
</p>

<p>
ANT is written in C++ and bears CPU cachelines in mind.  As such it is better suited for embedded systems than GPUs.
</p>

<p>
There are five directories.
</p>

<p>
  Data files for the examples.
</p>

<p>
NNm: This is the core of the system.  This includes implementations of ADAM and RPROP+ trainers.  There are also two loss functions.  For regression there is mean squared error (MSE).  For classification there is an implementation of Softmax.
</p>

<p>
Examples: Trival examples demonstrating NNm and DDL concepts.
</p>

<p>
Python: Bindings for Python and a trivial example.
</p>


